The Road to Modern AI: Multi-Layer Networks and Gradient Descent

• Multi-Layer Perceptrons (MLPs): Rosenblatt and others knew that expanding to a network of artificial neurons across multiple layers could solve nonlinearly separable problems. A simple three-neuron, two-layer network can solve the XOR problem by creating multiple linear decision boundaries in the first layer and combining them in the second.

• The Training Problem: The key challenge was that the perceptron learning rule did not generalize to multi-layer networks. A new algorithm was needed to simultaneously update all neuron weights in complex networks based on real data.

• Widrow and Hoff's LMS Algorithm (1959):
    ◦ Bernard Widrow and his student Ted Hoff at Stanford came "incredibly close" to solving the multi-layer training problem.
    ◦ They focused on gradient-based methods to mathematically measure and minimize the machine's error.
    ◦ They calculated the squared error between the target output and the machine's output.
    ◦ Their innovation was to use calculus (partial derivatives) to compute the "gradient"—the exact direction to move the weights to reduce the error—instead of numerically estimating it by testing neighborhood points.
    ◦ Their LMS (Least Mean Squares) algorithm updated weights proportionally to the error, making it more robust.
    ◦ However, Widrow and Hoff were unable to adapt LMS to train networks with multiple layers.

• The "Brick Wall" - Binary Activation Functions:
    ◦ The problem lay in the binary "all or nothing" artificial neuron model (developed by Pitts and McCulloch in the 1940s), which most researchers used.
    ◦ This model has a "step activation function" where the output is either 0 or 1.
    ◦ The slope of this function is zero everywhere (flat plateaus in the error landscape) and undefined at the origin (infinitely steep cliffs).
    ◦ This meant that when applying the chain rule (a calculus technique for multi-layer derivatives), the gradient would "snap to zero," preventing the incremental descent needed to find the minimum error.

• The Breakthrough: Smooth Activation Functions and Backpropagation:

    ◦ The "surprisingly simple" solution was to replace the "all or nothing" step activation function with a "smoother" function like the sigmoid function.
    ◦ A sigmoid function provides enough of a slope in its regions to guide the solution downhill, even in multi-layered error landscapes.
    ◦ In 1986, David Rumelhart, Jeff Hinton, and Ronald Williams published a paper presenting the modern backpropagation algorithm. They derived it as a generalization of the LMS algorithm (which they called the Delta Rule), using the chain rule with sigmoid activation functions to extend the gradient computation through multiple layers.

Modern Deep Learning: Rosenblatt's Vision Realized
• Scale of Modern Networks: The backpropagation algorithm enabled the training of massive neural networks. In 2020, OpenAI used backpropagation to train GPT-3, which had 175 billion learnable weights spread across 96 layers. Each layer contained "multi-layer perceptron" blocks, fulfilling the name Frank Rosenblatt gave them. GPT-3 effectively uses around 10 million artificial neurons. GPT-4 is reportedly around 10 times larger than GPT-3, bringing the neuron count to approximately 100 million.

• Pattern Recognition at Scale: 
Like the simple perceptron, GPT-4 is fundamentally a pattern recognizer, using its enormous network of neurons to learn complex patterns in language and predict what text comes next.
• The Atomic Unit: 
It is "incredible" that the perceptron, this "atomic unit," when connected in giant networks and trained with an extension of the LMS algorithm (backpropagation), has resulted in the most intelligent systems built to date.
• Rosenblatt's Legacy: 
Nearly 70 years after Frank Rosenblatt's initial, bold claims about the perceptron's future capabilities ("walk, talk, see, write, reproduce itself, and be conscious of its own existence"), time has increasingly proven his vision correct, despite missing some key details


-----


so how did rosenblat do the backpropagation or learning ? he did not use calculus

Frank Rosenblatt's method for training the Perceptron did not use backpropagation or calculus in the way modern neural networks do.
Here's how Rosenblatt's learning procedure worked:
• Perceptron Learning Rule (1957): Rosenblatt discovered this "simple procedure" to train his perceptron.
    ◦ The core idea was to adjust the "dials" (weights) based on whether the machine's output was correct or incorrect for a given pattern.
    ◦ If the output was desired to be positive but was not: The procedure instructed to turn up the dials for "on" switches and turn down the dials for "off" switches by a constant value called the learning rate.
    ◦ If the output was desired to be negative but was not: The procedure instructed to turn down the dials for "on" switches and turn up the dials for "off" switches by the learning rate.
    ◦ If the output was correct, the dials were left alone.
    ◦ This process was repeated with different patterns until the machine correctly classified all examples.
• Nature of the Algorithm: This was an "ad hoc method" compared to the later gradient-based approaches that explicitly used calculus. While effective for a single-layer perceptron on linearly separable problems—and even guaranteed to find a solution if one exists—the perceptron learning rule did not generalize to multi-layer networks.
The ability to train multi-layer networks using calculus (specifically, partial derivatives and the chain rule to compute gradients) and the backpropagation algorithm came much later, with key developments by Bernard Widrow and Ted Hoff (LMS algorithm, 1959) and the eventual breakthrough by David Rumelhart, Jeff Hinton, and Ronald Williams (modern backpropagation, 1986). Rosenblatt was working on multi-layer architectures before his death, but the suitable algorithm for learning their weights had not yet been found.
